{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b8d6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"C:/NIKEN/STIS/SEMESTER 5/Information Retrieval (P)/P2/berita\"\n",
    "for file in os.listdir(path):\n",
    "    if os.path.isfile(os.path.join(path, file)):\n",
    "        def read_text_file(file_path): \n",
    "            with open(file_path, 'r') as f:\n",
    "                return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a70e1202",
   "metadata": {},
   "outputs": [],
   "source": [
    "berita1 = read_text_file(f\"{path}\\\\berita1.txt\")\n",
    "berita2 = read_text_file(f\"{path}\\\\berita2.txt\")\n",
    "berita3 = read_text_file(f\"{path}\\\\berita3.txt\")\n",
    "berita4 = read_text_file(f\"{path}\\\\berita4.txt\")\n",
    "berita5 = read_text_file(f\"{path}\\\\berita5.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891336d2",
   "metadata": {},
   "source": [
    "## Okapi BM 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cd90882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc3': 1.1034159549468965, 'doc2': 0.7542259391529292, 'doc5': 0.5204049944065936}\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def tokenisasi(text):\n",
    "    tokens = text.split(\" \")\n",
    "    return tokens\n",
    "\n",
    "def stemming(text):\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    \n",
    "    #create stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    \n",
    "    #stemming process\n",
    "    output = stemmer.stem(text)\n",
    "    return output\n",
    "\n",
    "def stemming_sentence(text):\n",
    "    output = \"\"\n",
    "    for token in tokenisasi(text):\n",
    "        output = output + stemming(token) + \" \"\n",
    "    return output[:-1]\n",
    "\n",
    "doc_dict_raw = {}\n",
    "doc_dict_raw['doc1'] = berita1\n",
    "doc_dict_raw['doc2'] = berita2\n",
    "doc_dict_raw['doc3'] = berita3\n",
    "doc_dict_raw['doc4'] = berita4\n",
    "doc_dict_raw['doc5'] = berita5\n",
    "\n",
    "doc_dict = {}\n",
    "for doc_id, doc in doc_dict_raw.items():\n",
    "    doc_dict[doc_id] = stemming_sentence(doc)\n",
    "\n",
    "tokenized_corpus = [tokenisasi(doc_dict[doc_id]) for doc_id in doc_dict]\n",
    "\n",
    "'''EXACT TOP-K'''\n",
    "query = \"vaksin corona jakarta\"\n",
    "tokenized_query = tokenisasi(query)\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "from collections import OrderedDict\n",
    "def exact_top_k(doc_dict, rank_score, k):\n",
    "    relevance_scores = {}\n",
    "    i = 0\n",
    "    for doc_id in doc_dict.keys():\n",
    "        relevance_scores[doc_id] = rank_score[i]\n",
    "        i = i + 1\n",
    "        \n",
    "    sorted_value = OrderedDict(sorted(relevance_scores.items(), key = lambda x: x[1], reverse = True))\n",
    "    top_k = {j: sorted_value[j] for j in list(sorted_value)[:k]}\n",
    "    return top_k\n",
    "\n",
    "top_3 = exact_top_k(doc_dict, doc_scores, 3)\n",
    "print(top_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02776558",
   "metadata": {},
   "source": [
    "## Vector Space Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e22cf974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenisasi(text):\n",
    "    tokens = text.split(\" \")\n",
    "    return tokens\n",
    "\n",
    "def stemming(text):\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    # create stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    # stemming process\n",
    "    output = stemmer.stem(text)\n",
    "    return output\n",
    "\n",
    "def stemming_sentence(text):\n",
    "    output = \"\"\n",
    "    for token in tokenisasi(text):\n",
    "        output = output + stemming(token) + \" \"\n",
    "    return output[:-1]\n",
    "\n",
    "doc_dict_raw = {}\n",
    "doc_dict_raw['doc1'] = berita1\n",
    "doc_dict_raw['doc2'] = berita2\n",
    "doc_dict_raw['doc3'] = berita3\n",
    "doc_dict_raw['doc4'] = berita4\n",
    "doc_dict_raw['doc5'] = berita5\n",
    "\n",
    "doc_dict = {}\n",
    "for doc_id,doc in doc_dict_raw.items():\n",
    "    doc_dict[doc_id] = stemming_sentence(doc)\n",
    "\n",
    "vocab = []\n",
    "inverted_index = {}\n",
    "for doc_id,doc in doc_dict.items():\n",
    "    for token in tokenisasi(doc):\n",
    "        if token not in vocab:\n",
    "            vocab.append(token)\n",
    "            inverted_index[token] = []\n",
    "        if token in inverted_index:\n",
    "            if doc_id not in inverted_index[token]:\n",
    "                inverted_index[token].append(doc_id)\n",
    "\n",
    "query = \"vaksin corona jakarta\"\n",
    "def termFrequency(vocab, query):\n",
    "    tf_query = {}\n",
    "    for word in vocab:\n",
    "        tf_query[word] = query.count(word)\n",
    "    return tf_query\n",
    "tf_query = termFrequency(vocab, query)\n",
    "\n",
    "def wordDocFre(vocab, doc_dict):\n",
    "    df = {}\n",
    "    for word in vocab:\n",
    "        frq = 0\n",
    "        for doc in doc_dict.values():\n",
    "            if word in tokenisasi(doc):\n",
    "                frq = frq + 1\n",
    "        df[word] = frq\n",
    "    return df\n",
    "\n",
    "'''TERM-FREQUENCY'''\n",
    "def termFrequencyInDoc(vocab, doc_dict) :\n",
    "    tf_docs = {}\n",
    "    for doc_id in doc_dict.keys() :\n",
    "        tf_docs[doc_id] = {}\n",
    "    for word in vocab :\n",
    "        for doc_id, doc in doc_dict.items() :\n",
    "            tf_docs[doc_id][word] = doc.count(word)\n",
    "    return tf_docs\n",
    "\n",
    "import numpy as np\n",
    "def inverseDocFre(vocab, doc_fre, length):\n",
    "    idf = {}\n",
    "    for word in vocab:\n",
    "        #idf[word]  = np.log((length) / doc_fre[word])\n",
    "        idf[word]  = 1 + np.log10((length + 1) / (doc_fre[word]+1))\n",
    "    return idf\n",
    "\n",
    "idf = inverseDocFre(vocab, wordDocFre(vocab, doc_dict), len(doc_dict))\n",
    "\n",
    "# Term - Query Matrix\n",
    "TQ = np.zeros((len(vocab), 1)) #hanya 1 query\n",
    "for word in vocab:\n",
    "    ind1 = vocab.index(word)\n",
    "    TQ[ind1][0] = tf_query[word]*idf[word]\n",
    "\n",
    "'''TF-IDF'''\n",
    "def tfidf(vocab, tf, idf_scr, doc_dict) :\n",
    "    tf_idf_scr = {}\n",
    "    for doc_id in doc_dict.keys() :\n",
    "        tf_idf_scr[doc_id] = {}\n",
    "    for word in vocab :\n",
    "        for doc_id, doc in doc_dict.items() :\n",
    "            tf_idf_scr[doc_id][word] = tf[doc_id][word] * idf_scr[word]\n",
    "    return tf_idf_scr\n",
    "\n",
    "tf_idf = tfidf(vocab, termFrequencyInDoc(vocab, doc_dict), inverseDocFre(vocab, wordDocFre(vocab, doc_dict), len(doc_dict)), doc_dict)\n",
    "    \n",
    "'''TERM-DOCUMENT MATRIX'''\n",
    "TD = np.zeros((len(vocab), len(doc_dict)))\n",
    "for word in vocab :\n",
    "    for doc_id, doc in tf_idf.items() :\n",
    "        ind1 = vocab.index(word)\n",
    "        ind2 = list(tf_idf.keys()).index(doc_id)\n",
    "        TD[ind1][ind2] = tf_idf[doc_id][word]\n",
    "        \n",
    "'''COSINE SIMILARITY'''\n",
    "import math\n",
    "def cosine_sim(vec1, vec2) :\n",
    "    vec1 = list(vec1)\n",
    "    vec2 = list(vec2)\n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1) :\n",
    "        dot_prod += v * vec2[i]\n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "    return dot_prod / (mag_1 * mag_2)\n",
    "\n",
    "'''EXACT TOP-K'''\n",
    "idf = inverseDocFre(vocab,wordDocFre(vocab,doc_dict),len(doc_dict))\n",
    "query = \"vaksin corona jakarta\"\n",
    "def termFrequency(vocab, query):\n",
    "    tf_query = {}\n",
    "    for word in vocab:\n",
    "        tf_query[word] = query.count(word)\n",
    "    return tf_query\n",
    "tf_query = termFrequency(vocab, query)\n",
    "\n",
    "from collections import OrderedDict\n",
    "def exact_top_k1(doc_dict, TD, q, k):\n",
    "    relevance_scores = {}\n",
    "    i = 0\n",
    "    for doc_id in doc_dict.keys():\n",
    "        relevance_scores[doc_id] = cosine_sim(q, TD[:, i])\n",
    "        i = i + 1\n",
    "    sorted_value = OrderedDict(sorted(relevance_scores.items(), key=lambda x: x[1], reverse = True))\n",
    "    top_k = {j: sorted_value[j] for j in list(sorted_value)[:k]}\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "303b4bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VECTOR SPACE MODEL TOP-3\n",
      "{'doc2': 0.16640342749946516, 'doc3': 0.13671100189084426, 'doc5': 0.06326722354821664}\n"
     ]
    }
   ],
   "source": [
    "top_3_2 = exact_top_k1(doc_dict, TD, TQ[:, 0], 3)\n",
    "print(\"VECTOR SPACE MODEL TOP-3\")\n",
    "print(top_3_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e749a10",
   "metadata": {},
   "source": [
    "## Likelihood Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03c5b0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc1': 0.0, 'doc2': 0.0, 'doc3': 3.756574004507889e-06, 'doc4': 0.0, 'doc5': 0.0}\n"
     ]
    }
   ],
   "source": [
    "likelihood_scores = {}\n",
    "vocab = set()\n",
    "for doc_id in doc_dict.keys():\n",
    "    likelihood_scores[doc_id] = 1\n",
    "    tokens = tokenisasi(doc_dict[doc_id])\n",
    "    vocab.update(tokens)\n",
    "    for q in tokenized_query:\n",
    "        likelihood_scores[doc_id]=likelihood_scores[doc_id]*tokens.count(q)/len(tokens)\n",
    "\n",
    "print(likelihood_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ab925e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc3': 3.756574004507889e-06, 'doc1': 0.0, 'doc2': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "sorted_value = OrderedDict(sorted(likelihood_scores.items(), key=lambda x: x[1], reverse = True))\n",
    "top_3 = {j: sorted_value[j] for j in list(sorted_value)[:3]}\n",
    "print(top_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04415c5a",
   "metadata": {},
   "source": [
    "#### Membuat Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e3fc886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fauci', 'mobilitas', 'detik', 'dokter', 'hijau', 'wilayah', 'oleh', 'tahap', 'harus', 'com', 'satgas', 'jawab', 'panas', 'kamu', 'level', 'yang', 'longgar', 'kait', 'nasihat', '165', 'memang', 'djoerban', 'ketua', 'area', 'asal', 'cs', 'sehat', 'rutin', '90', 'laut', 'belum', 'lantas', 'perlu', 'ppkm', 'untuk', 'd-5816582', 'tahu', 'dr', 'pasca', 'zona', 'batas', 'dadak', 'suntik', 'jika', 'apa', '24', 'prof', 'sebut', 'timur', 'corona', 'wilayah-kamu-sudah-bebas-covid-19-cek-34-kabkota-zona-hijau-terbaru', 'senin', 'musim', '1', 'vaksinasi', 'vaksin', 'langsung', 'ada', 'gantung', 'atau', 'ri', 'padahal', 'total', '11', 'picu', 'utara', 'puncak', 'jenis', 'usai', 'covid-19', '2021', 'siti', 'tidak', 'tengah', 'alami', 'virus', 'kota', 'kemudian', 'cukup', 'kembali', 'cegah', 'kemenkes', 'pakar', 'balitbangkes', 'efektivitas', 'juga', '13', 'varian', 'tular', 'mulai', 'pfizer', 'amerika', 'kab', 'november', 'medis', 'sekali', 'tingkat', 'sudah', 'bakal', 'seperti', 'ikut', 'alert', 'masyarakat', 'desember', 'laku', 'ia', '34', '-', 'akhir', 'terap', '327', 'd-5816690', 'bijak', 'guna', 'tiga', 'ini', 'bebas', 'gelombang', 'januari', 'kini', 'berita-detikhealth', 'nasional', 'd-5813949', '2022', 'sempat', 'idi', 'per', 'jawa', 'protokol', 'umum', 'turun', 'hitung', 'ampuh', 'strain', 'sulawesi', '86', 'sakit', 'riset', 'tiap', 'masih', 'tambah', 'gedung', 'giat', 'd-5812940', 'kendali', 'dasar', 'direktur', 'banyak', 'menteri', 'putih', 'dalam', 'zubairi', 'tahun', 'baru', 'dosis', 'delta', 'lawan', 'vaksin-covid-19-bakal-rutin-setiap-tahun-tergantung-ini-penjelasannya', 'jadi', 'hingga', 'perintah', 'indonesia', 'p2pml', 'kaji', 'apakah', 'mungkin', 'tarmizi', 'data', 'https', 'ingat', 'jumlah', 'influenza', 'd-5816534', 'rencana', 'turut', '15', 'bukti', 'stabil', 'satu', 'bulan', 'namun', 'beri', 'reda', 'anthony', 'dari', 'barat', 'minggu', 'dki', 'jakarta', 'signifikan', 'hadap', 'alpha', 'persen', 'di', 'catat', 'moderna', 'jelas', 'lagi', 'dengan', 'corona-di-as-mendadak-naik-lagi-usai-serangan-delta-sempat-mereda', 'awal', 'sampai', 'cek', '2', 'kasus', 'lalu', 'ri-mulai-suntikkan-booster-di-2022-masihkah-ampuh-lawan-varian-delta-cs', 'pasien', 'naik', 'hal', 'dan', 'kepala', '3', 'as', '1-2', 'ikat', 'sementara', 'nadia', 'health', '57', 'aku', 'pasti', 'tanya', 'pada', 'beta', 'alert-kasus-varian-delta-covid-19-di-dki-meningkat', 'serang', 'singgung', 'serikat', 'tunjuk', 'booster', 'jauh'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = [j for sub in [tokenisasi(doc_dict[doc_id]) for \n",
    "doc_id in doc_dict] for j in sub]\n",
    "vocab = set(tokenized_corpus)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c74435",
   "metadata": {},
   "source": [
    "### Likelihood Model dengan Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aecd0ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc1': 5.990490455830797e-08, 'doc2': 2.4644382333070736e-07, 'doc3': 5.999746810684589e-07, 'doc4': 1.5402738958169208e-07, 'doc5': 1.1870046611002282e-07}\n"
     ]
    }
   ],
   "source": [
    "alpha = 1\n",
    "likelihood_scores_laplace = {}\n",
    "for doc_id in doc_dict.keys():\n",
    "    likelihood_scores_laplace[doc_id] = 1\n",
    "    tokens = tokenisasi(doc_dict[doc_id])\n",
    "    for q in tokenized_query:\n",
    "        likelihood_scores_laplace[doc_id]=likelihood_scores_laplace[doc_id]*(tokens.count(q)+alpha)/(len(tokens)+len(vocab)*alpha)\n",
    "\n",
    "print(likelihood_scores_laplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d0f449c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc3': 5.999746810684589e-07, 'doc2': 2.4644382333070736e-07, 'doc4': 1.5402738958169208e-07}\n"
     ]
    }
   ],
   "source": [
    "sorted_value = OrderedDict(sorted(likelihood_scores_laplace.items(), key=lambda x: x[1], reverse = True))\n",
    "top_3 = {j: sorted_value[j] for j in list(sorted_value)[:3]}\n",
    "print(top_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df83198",
   "metadata": {},
   "source": [
    "### Likelihood Model dengan Jelinek-Mercer Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b79cd392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc1': 2.6190692275527747e-07, 'doc2': 7.97496704914498e-07, 'doc3': 2.758125277898707e-06, 'doc4': 5.844033998799556e-07, 'doc5': 9.270498351757131e-07}\n"
     ]
    }
   ],
   "source": [
    "lamda = 0.5\n",
    "likelihood_scores_mercer = {}\n",
    "for doc_id in doc_dict.keys():\n",
    "    likelihood_scores_mercer[doc_id] = 1\n",
    "    tokens = tokenisasi(doc_dict[doc_id])\n",
    "    for q in tokenized_query:\n",
    "        likelihood_scores_mercer[doc_id]=likelihood_scores_mercer[doc_id]*((lamda*tokens.count(q)/len(tokens))+((1-lamda)*tokenized_corpus.count(q)/len(tokenized_corpus)))\n",
    "\n",
    "print(likelihood_scores_mercer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60969fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc3': 2.758125277898707e-06, 'doc5': 9.270498351757131e-07, 'doc2': 7.97496704914498e-07}\n"
     ]
    }
   ],
   "source": [
    "sorted_value = OrderedDict(sorted(likelihood_scores_mercer.items(), key=lambda x: x[1], reverse = True))\n",
    "top_3 = {j: sorted_value[j] for j in list(sorted_value)[:3]}\n",
    "print(top_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b299d5",
   "metadata": {},
   "source": [
    "### Likelihood Model dengan Dirichlet Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f4df4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc1': 3.931902225004399e-10, 'doc2': 3.8300390580787434e-08, 'doc3': 3.73857765656231e-06, 'doc4': 1.6938850281982751e-09, 'doc5': 4.4630340210217655e-08}\n"
     ]
    }
   ],
   "source": [
    "miu = 2\n",
    "likelihood_scores_dirichlet = {}\n",
    "for doc_id in doc_dict.keys():\n",
    "    likelihood_scores_dirichlet[doc_id] = 1\n",
    "    tokens = tokenisasi(doc_dict[doc_id])\n",
    "    for q in tokenized_query:\n",
    "        likelihood_scores_dirichlet[doc_id]=likelihood_scores_dirichlet[doc_id]*(tokens.count(q)+miu*tokenized_corpus.count(q)/len(tokenized_corpus))/(len(tokens)+miu)\n",
    "\n",
    "print(likelihood_scores_dirichlet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d8546e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc3': 3.73857765656231e-06, 'doc5': 4.4630340210217655e-08, 'doc2': 3.8300390580787434e-08}\n"
     ]
    }
   ],
   "source": [
    "sorted_value = OrderedDict(sorted(likelihood_scores_dirichlet.items(), key=lambda x: x[1], reverse = True))\n",
    "top_3 = {j: sorted_value[j] for j in list(sorted_value)[:3]}\n",
    "print(top_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9612e672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
